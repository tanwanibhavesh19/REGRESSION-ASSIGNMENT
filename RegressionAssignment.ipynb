{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**REGRESSION ASSIGNMENT**\n",
        "\n",
        "ASSIGNMENT QUESTIONS\n",
        "\n",
        "1. What is Simple Linear Regression ?\n",
        "\n",
        "  - Simple linear regression is a statistical method that models the relationship between two variables using a straight line.\n",
        "  - It aims to find the best-fitting line that describes the correlation between an independent variable (X) and a dependent variable (Y).\n",
        "  - This line can be used to predict or estimate the value of Y based on a given value of X.\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression ?\n",
        "\n",
        "  - The key assumptions of simple linear regression include linearity, independence of errors, homoscedasticity, and normality of errors.\n",
        "  - These assumptions ensure the validity and reliability of the regression model's results.\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y=mX+c ?\n",
        "\n",
        "  - In the equation y = mx + c, the coefficient 'm' represents the slope or gradient of the line. It indicates how steeply the line rises or falls as you move along the x-axis.\n",
        "  - A positive 'm' indicates an upward slope, while a negative 'm' indicates a downward slope.\n",
        "\n",
        "4. What does the intercept c represent in the equation Y=mX+c ?\n",
        "\n",
        "  - In the equation y = mx + c, the term 'c' represents the y-intercept of the line.\n",
        "  - This means it's the point where the line crosses the y-axis on a graph.\n",
        "  - More specifically, it's the y-coordinate of that point when the x-coordinate is zero.\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression ?\n",
        "\n",
        "  - In simple linear regression, the slope m can be calculated using the formula: m = (n * ∑xy - (∑x) * (∑y)) / (n * ∑x² - (∑x)²).\n",
        "  - This formula uses the sum of x values, the sum of y values, the sum of the product of x and y values, the sum of the squares of x values, and the number of data points (n).\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression ?\n",
        "\n",
        "  - The least squares method in simple linear regression is used to find the \"line of best fit\" for a set of data points.\n",
        "  - It does this by minimizing the sum of the squared vertical distances (errors) between each data point and the line.\n",
        "  - In essence, it aims to find the straight line that best represents the relationship between the independent and dependent variables in a given dataset.\n",
        "\n",
        "\n",
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression ?\n",
        "\n",
        "  - In Simple Linear Regression, the Coefficient of Determination (R²) indicates the proportion of variance in the dependent variable that is explained by the independent variable in the model.\n",
        "\n",
        "\n",
        "8. What is Multiple Linear Regression ?\n",
        "\n",
        "  - Multiple linear regression is a statistical method used to model the relationship between a dependent variable and two or more independent variables.\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression ?\n",
        "\n",
        "  - The primary distinction between simple and multiple linear regression lies in the number of independent variables used to predict a dependent variable. - Simple linear regression utilizes one independent variable, while multiple linear regression incorporates two or more.\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression ?\n",
        "\n",
        "  - The key assumptions of multiple linear regression include a linear relationship between the dependent and independent variables, independent observations, no multicollinearity among independent variables, homoscedasticity (constant variance of errors), no autocorrelation (errors independent), and multivariate normality of the dependent variable (errors).\n",
        "\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model ?\n",
        "\n",
        "  - Heteroscedasticity in a multiple linear regression model refers to a situation where the variance of the error terms is not constant across all observations.\n",
        "\n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity ?\n",
        "\n",
        "  - To improve a multiple linear regression model with high multicollinearity, consider these strategies: remove correlated predictors, combine them into a single variable, use regularization techniques (like ridge or Lasso), or employ dimensionality reduction methods like Principal Component Analysis (PCA).\n",
        "\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models ?\n",
        "\n",
        "  - Several techniques can transform categorical variables for use in regression models. One-hot encoding is commonly used, creating binary variables for each category.\n",
        "  - Label encoding assigns numerical labels to categories.\n",
        "  - Ordinal encoding is suitable for variables with a meaningful order, and target encoding replaces categories with the mean target value.\n",
        "\n",
        "14. What is the role of interaction terms in Multiple Linear Regression ?\n",
        "\n",
        "  - In multiple linear regression, interaction terms represent a non-additive relationship between two or more independent variables and the dependent variable.\n",
        "  - They allow the model to capture situations where the effect of one predictor on the outcome changes depending on the level of another predictor.\n",
        "\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression ?\n",
        "\n",
        "  - The interpretation of the intercept in simple and multiple linear regression is similar: it represents the expected value of the dependent variable when all independent variables are zero.\n",
        "  - However, the context of the intercept's interpretation can be different in multiple regression due to the presence of multiple predictors.\n",
        "\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions ?\n",
        "\n",
        "  - In regression analysis, the slope represents the change in the dependent variable (y) for every one-unit change in the independent variable (x).\n",
        "  - It indicates the strength and direction of the relationship between the two variables.\n",
        "\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables ?\n",
        "\n",
        "  - The intercept in a regression model, representing the value of the dependent variable when all independent variables are zero, provides a baseline or starting point for understanding the relationship between variables.\n",
        "  - It helps contextualize the model by showing the expected value of the outcome variable when no other predictor variables are in effect.\n",
        "\n",
        "18. What are the limitations of using R² as a sole measure of model performance ?\n",
        "\n",
        "  - Limitations of R-Squared:\n",
        "  - However, it doesn't tell you whether your chosen model is good or bad, nor will it tell you whether the data and predictions are biased.\n",
        "  - A high or low R-squared isn't necessarily good or bad—it doesn't convey the reliability of the model or whether you've chosen the right regression.\n",
        "\n",
        "19. How would you interpret a large standard error for a regression coefficient ?\n",
        "\n",
        "  - A large standard error for a regression coefficient suggests that the estimated coefficient is imprecise and potentially unreliable.\n",
        "  - It indicates that the coefficient's value might vary significantly if the model were re-estimated on a different sample of data.\n",
        "  - This means there's a higher degree of uncertainty surrounding the estimate of the coefficient's true population value.\n",
        "\n",
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it ?\n",
        "\n",
        "  - Heteroscedasticity, or non-constant variance of errors, in residual plots is typically identified by the presence of a funnel or cone shape.\n",
        "  - It's important to address because it violates the assumptions of OLS regression, leading to unreliable standard errors and confidence intervals.\n",
        "\n",
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R² ?\n",
        "\n",
        "  - A high R² with a low Adjusted R² in a multiple linear regression model indicates that while the model as a whole explains a good portion of the variance in the dependent variable, it might be overfitting the data due to the inclusion of many (or possibly not very useful) independent variables.\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression ?\n",
        "\n",
        "  - Scaling variables in multiple linear regression is crucial for several reasons, including improved model convergence, clearer interpretation of coefficients, and better performance of algorithms that rely on distance or penalties based on coefficient magnitudes.\n",
        "\n",
        "23. What is polynomial regression ?\n",
        "\n",
        "  - Polynomial regression is a type of regression analysis where the relationship between an independent variable (x) and a dependent variable (y) is modeled as an nth-degree polynomial.\n",
        "\n",
        "\n",
        "24. How does polynomial regression differ from linear regression ?\n",
        "\n",
        "  - Polynomial regression extends linear regression by allowing for non-linear relationships between variables.\n",
        "  - While linear regression models a straight line, polynomial regression fits a curve to the data, capturing potentially more complex relationships.\n",
        "\n",
        "\n",
        "25. When is polynomial regression used ?\n",
        "\n",
        "  - Polynomial regression is used when the relationship between an independent variable and a dependent variable is not linear, but rather curved or non-linear.\n",
        "\n",
        "\n",
        "26. What is the general equation for polynomial regression ?\n",
        "\n",
        "  - The general equation for polynomial regression, a method used to fit a polynomial of a specific degree to a dataset, is:\n",
        "  - y = β₀ + β₁x + β₂x² + ... + βₙxⁿ + ε\n",
        "\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables ?\n",
        "\n",
        "  - Yes, polynomial regression can be applied to multiple variables.\n",
        "  - In essence, you treat the powers of the variables as distinct variables in a multiple regression model.\n",
        "  - This allows you to capture more complex, non-linear relationships between the independent variables and the dependent variable.\n",
        "\n",
        "28. What are the limitations of polynomial regression ?\n",
        "\n",
        "  - Polynomial regression, while powerful, has limitations including overfitting, sensitivity to outliers, and challenges with interpretability, especially with higher-degree polynomials. Additionally, the computational cost increases with the degree of the polynomial.\n",
        "  - Choosing the optimal polynomial degree can also be difficult, requiring techniques like cross-validation.\n",
        "\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial ?\n",
        "\n",
        "  - Several methods can be used to evaluate model fit when selecting the degree of a polynomial, including R-squared, cross-validation, residual plots, and visual inspection.\n",
        "\n",
        "\n",
        "30. Why is visualization important in polynomial regression ?\n",
        "\n",
        "  - Visualization is crucial in polynomial regression to understand and assess the model's performance and the relationships within the data.\n",
        "\n",
        "\n",
        "31. How is polynomial regression implemented in Python ?\n",
        "\n",
        "  - Polynomial regression is implemented in Python using the scikit-learn library.\n",
        "  - It extends linear regression by adding polynomial terms to the model, allowing it to fit non-linear relationships between variables."
      ],
      "metadata": {
        "id": "VOm-XA7wvMon"
      }
    }
  ]
}